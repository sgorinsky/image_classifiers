{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing as other notebook except conv net was constructed from grey images. As you can see, basic neural nets overfitted tremendously so the conv net was a massive improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports for our work\n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from tflearn.metrics import Accuracy\n",
    "\n",
    "RANDOM_SEED = 9999\n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading and formatting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatsDogs:\n",
    "# Documentation on npy binary format for saving numpy arrays for later use\n",
    "#     https://towardsdatascience.com/\n",
    "#             why-you-should-start-using-npy-file-more-often-df2a13cc0161\n",
    "# Under the working directory, data files are in directory cats_dogs_64_128 \n",
    "# Read in cats and dogs grayscale 64x64 files to create training data\n",
    "\n",
    "# 1000 dog and 1000 cat images\n",
    "# greyscale: each image is a 128x128x1 pixel array (the last dim is the color)\n",
    "cats_1000_128_128_1 = np.load('cats_dogs_images/greyscale/cats_1000_128_128_1.npy')\n",
    "dogs_1000_128_128_1 = np.load('cats_dogs_images/greyscale/dogs_1000_128_128_1.npy')\n",
    "\n",
    "# color: each image is a 64x64x3 pixel array (the \"x3\" is the color for RGB dimensions)\n",
    "cats_1000_64_64_3 = np.load('cats_dogs_images/color/cats_1000_64_64_3.npy')\n",
    "dogs_1000_64_64_3 = np.load('cats_dogs_images/color/dogs_1000_64_64_3.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt  # for display of images\n",
    "def show_grayscale_image(image):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def show_color_image(image):\n",
    "    plt.imshow(image) \n",
    "    plt.axis('off')\n",
    "    plt.show()   \n",
    "\n",
    "# Examine first cat and first dog grayscale images\n",
    "show_grayscale_image(cats_1000_128_128_1[0,:,:,0])\n",
    "show_grayscale_image(dogs_1000_128_128_1[0,:,:,0])\n",
    "\n",
    "show_color_image(cats_1000_64_64_3[0,:,:,0])\n",
    "show_color_image(dogs_1000_64_64_3[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work the data for cats and dogs numpy arrays \n",
    "# These numpy arrays were generated in previous data prep work\n",
    "# Stack the numpy arrays for the inputs\n",
    "\n",
    "X_cat_dog_grey = np.concatenate((cats_1000_128_128_1, dogs_1000_128_128_1), axis = 0)\n",
    "\n",
    "X_cat_dog_color = np.concatenate((cats_1000_64_64_3, dogs_1000_64_64_3), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 64, 64, 3)\n",
      "(2000, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_cat_dog_color.shape)\n",
    "print(X_cat_dog_grey.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get an array of 2000 arrays of length 16384\n",
    "X_cat_dog_grey = X_cat_dog_grey.reshape(-1, 128*128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Min-max:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn for min-max scaling of the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.array([0., 255.]).reshape(-1,1))\n",
    "X_cat_dog_grey_min_max = scaler.transform(X_cat_dog_grey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16384,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat_dog_grey_min_max[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit Learn for Robust Scaling scaling of the data\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler2 = RobustScaler()\n",
    "scaler2.fit(np.array([0., 255.]).reshape(-1,1)) \n",
    "X_cat_dog_grey_robust = scaler2.transform(X_cat_dog_grey.reshape(-1,128*128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30980392,  0.35686275,  0.36470588, ..., -0.98431373,\n",
       "       -0.98431373, -0.98431373])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat_dog_grey_robust[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels to be used 1000 cats = 0 1000 dogs = 1\n",
    "y_cat_dog = np.concatenate((np.zeros((1000), dtype = np.int32), \n",
    "                      np.ones((1000), dtype = np.int32)), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting into training sets by min-max and robust scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max Split\n",
    "# training (80%) and test (20%)  \n",
    "X_train_min_max, X_test_min_max, y_train_min_max, y_test_min_max = \\\n",
    "    train_test_split(X_cat_dog_grey_min_max, y_cat_dog, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Split\n",
    "# training (80%) and test (20%)  \n",
    "X_train_r, X_test_r, y_train_r, y_test_r = \\\n",
    "    train_test_split(X_cat_dog_grey_robust, y_cat_dog, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal train-test split and reshaping for convolutional neural net at the end\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_cat_dog_grey, y_cat_dog, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 128, 128, 1)\n",
    "X_test = X_test.reshape(-1, 128, 128, 1)\n",
    "\n",
    "# leaves 2 options that the conv net can choose from\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 128, 128, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up tflow models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup - define neuron layer\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set height & width\n",
    "height = 128\n",
    "width = 128 \n",
    "\n",
    "# 300 and 100 nodes for layers 1 and 2 as used with MNIST from Geron\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "\n",
    "\n",
    "channels = 1  # When working with color images use channels = 3\n",
    "\n",
    "n_inputs = height * width\n",
    "\n",
    "#CatsDogs# Has two output values # MNIST had ten digits n_outputs = 10  \n",
    "n_outputs = 2  # binary classification for Cats and Dogs, 1 output node 0/1\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# dnn... Deep neural network model from Geron Chapter 10\n",
    "# Note that this model makes no use of the fact that we have\n",
    "# pixel data arranged in rows and columns\n",
    "# So a 64x64 matrix of raster values becomes a vector of 4096 input variables\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\",\n",
    "                           activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                           activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50 epochs using min-max scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.61 Test accuracy: 0.485\n",
      "1 Train accuracy: 0.6 Test accuracy: 0.485\n",
      "2 Train accuracy: 0.61 Test accuracy: 0.485\n",
      "3 Train accuracy: 0.62 Test accuracy: 0.495\n",
      "4 Train accuracy: 0.62 Test accuracy: 0.4925\n",
      "5 Train accuracy: 0.63 Test accuracy: 0.5\n",
      "6 Train accuracy: 0.65 Test accuracy: 0.5025\n",
      "7 Train accuracy: 0.68 Test accuracy: 0.505\n",
      "8 Train accuracy: 0.67 Test accuracy: 0.5175\n",
      "9 Train accuracy: 0.68 Test accuracy: 0.5125\n",
      "10 Train accuracy: 0.7 Test accuracy: 0.5325\n",
      "11 Train accuracy: 0.72 Test accuracy: 0.5325\n",
      "12 Train accuracy: 0.69 Test accuracy: 0.5275\n",
      "13 Train accuracy: 0.69 Test accuracy: 0.5225\n",
      "14 Train accuracy: 0.69 Test accuracy: 0.525\n",
      "15 Train accuracy: 0.7 Test accuracy: 0.54\n",
      "16 Train accuracy: 0.74 Test accuracy: 0.55\n",
      "17 Train accuracy: 0.69 Test accuracy: 0.535\n",
      "18 Train accuracy: 0.73 Test accuracy: 0.53\n",
      "19 Train accuracy: 0.66 Test accuracy: 0.525\n",
      "20 Train accuracy: 0.77 Test accuracy: 0.545\n",
      "21 Train accuracy: 0.71 Test accuracy: 0.54\n",
      "22 Train accuracy: 0.78 Test accuracy: 0.55\n",
      "23 Train accuracy: 0.77 Test accuracy: 0.5475\n",
      "24 Train accuracy: 0.79 Test accuracy: 0.545\n",
      "25 Train accuracy: 0.81 Test accuracy: 0.5525\n",
      "26 Train accuracy: 0.76 Test accuracy: 0.5975\n",
      "27 Train accuracy: 0.77 Test accuracy: 0.57\n",
      "28 Train accuracy: 0.78 Test accuracy: 0.58\n",
      "29 Train accuracy: 0.79 Test accuracy: 0.5475\n",
      "30 Train accuracy: 0.77 Test accuracy: 0.55\n",
      "31 Train accuracy: 0.83 Test accuracy: 0.555\n",
      "32 Train accuracy: 0.67 Test accuracy: 0.615\n",
      "33 Train accuracy: 0.81 Test accuracy: 0.585\n",
      "34 Train accuracy: 0.77 Test accuracy: 0.56\n",
      "35 Train accuracy: 0.74 Test accuracy: 0.59\n",
      "36 Train accuracy: 0.83 Test accuracy: 0.55\n",
      "37 Train accuracy: 0.86 Test accuracy: 0.5525\n",
      "38 Train accuracy: 0.69 Test accuracy: 0.525\n",
      "39 Train accuracy: 0.76 Test accuracy: 0.59\n",
      "40 Train accuracy: 0.85 Test accuracy: 0.58\n",
      "41 Train accuracy: 0.76 Test accuracy: 0.585\n",
      "42 Train accuracy: 0.85 Test accuracy: 0.5725\n",
      "43 Train accuracy: 0.66 Test accuracy: 0.51\n",
      "44 Train accuracy: 0.74 Test accuracy: 0.5325\n",
      "45 Train accuracy: 0.85 Test accuracy: 0.585\n",
      "46 Train accuracy: 0.87 Test accuracy: 0.55\n",
      "47 Train accuracy: 0.78 Test accuracy: 0.555\n",
      "48 Train accuracy: 0.68 Test accuracy: 0.5325\n",
      "49 Train accuracy: 0.87 Test accuracy: 0.5575\n",
      "CPU times: user 1min 44s, sys: 4.92 s, total: 1min 49s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "init = tf.global_variables_initializer()    \n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(y_train_min_max.shape[0] // batch_size):\n",
    "            X_batch_min_max = X_train_min_max[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch_min_max = y_train_min_max[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch_min_max, y: y_batch_min_max})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch_min_max, y: y_batch_min_max})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test_min_max, y: y_test_min_max})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        save_path = saver.save(sess, \"./my_catdog_model_min_max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50 epochs using robust scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.79 Test accuracy: 0.505\n",
      "1 Train accuracy: 0.91 Test accuracy: 0.51\n",
      "2 Train accuracy: 0.92 Test accuracy: 0.525\n",
      "3 Train accuracy: 0.94 Test accuracy: 0.5375\n",
      "4 Train accuracy: 0.95 Test accuracy: 0.5375\n",
      "5 Train accuracy: 0.97 Test accuracy: 0.5425\n",
      "6 Train accuracy: 0.97 Test accuracy: 0.5425\n",
      "7 Train accuracy: 0.97 Test accuracy: 0.55\n",
      "8 Train accuracy: 1.0 Test accuracy: 0.55\n",
      "9 Train accuracy: 1.0 Test accuracy: 0.5575\n",
      "10 Train accuracy: 1.0 Test accuracy: 0.5525\n",
      "11 Train accuracy: 1.0 Test accuracy: 0.5575\n",
      "12 Train accuracy: 1.0 Test accuracy: 0.5625\n",
      "13 Train accuracy: 1.0 Test accuracy: 0.5575\n",
      "14 Train accuracy: 1.0 Test accuracy: 0.5625\n",
      "15 Train accuracy: 1.0 Test accuracy: 0.5625\n",
      "16 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "17 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "18 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "19 Train accuracy: 1.0 Test accuracy: 0.5575\n",
      "20 Train accuracy: 1.0 Test accuracy: 0.5525\n",
      "21 Train accuracy: 1.0 Test accuracy: 0.5475\n",
      "22 Train accuracy: 1.0 Test accuracy: 0.5525\n",
      "23 Train accuracy: 1.0 Test accuracy: 0.5575\n",
      "24 Train accuracy: 1.0 Test accuracy: 0.555\n",
      "25 Train accuracy: 1.0 Test accuracy: 0.555\n",
      "26 Train accuracy: 1.0 Test accuracy: 0.555\n",
      "27 Train accuracy: 1.0 Test accuracy: 0.555\n",
      "28 Train accuracy: 1.0 Test accuracy: 0.5575\n",
      "29 Train accuracy: 1.0 Test accuracy: 0.5525\n",
      "30 Train accuracy: 1.0 Test accuracy: 0.5525\n",
      "31 Train accuracy: 1.0 Test accuracy: 0.555\n",
      "32 Train accuracy: 1.0 Test accuracy: 0.555\n",
      "33 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "34 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "35 Train accuracy: 1.0 Test accuracy: 0.555\n",
      "36 Train accuracy: 1.0 Test accuracy: 0.5575\n",
      "37 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "38 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "39 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "40 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "41 Train accuracy: 1.0 Test accuracy: 0.5575\n",
      "42 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "43 Train accuracy: 1.0 Test accuracy: 0.5625\n",
      "44 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "45 Train accuracy: 1.0 Test accuracy: 0.5625\n",
      "46 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "47 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "48 Train accuracy: 1.0 Test accuracy: 0.56\n",
      "49 Train accuracy: 1.0 Test accuracy: 0.5575\n",
      "CPU times: user 1min 54s, sys: 5.31 s, total: 1min 59s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "init = tf.global_variables_initializer()  \n",
    "\n",
    "startTime = time.time()\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(y_train_r.shape[0] // batch_size):\n",
    "            X_batch_r = X_train_r[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch_r = y_train_r[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            sess.run(training_op, feed_dict={X: X_batch_r, y: y_batch_r})\n",
    "        acc_train_3 = accuracy.eval(feed_dict={X: X_batch_r, y: y_batch_r})\n",
    "        acc_test_3 = accuracy.eval(feed_dict={X: X_test_r, y: y_test_r})\n",
    "        print(epoch, \"Train accuracy:\", acc_train_3, \"Test accuracy:\", acc_test_3)\n",
    "\n",
    "        save_path = saver.save(sess, \"./my_catdog_model_robust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "## input\n",
    "input_layer = input_data(shape=[None, 128, 128, 1], name='input') # dimensions of each of the imgs\n",
    "\n",
    "## middle layers\n",
    "conv1 = conv_2d(input_layer, 64, 4, activation='relu')\n",
    "net = max_pool_2d(conv1, 5)\n",
    "\n",
    "conv2 = conv_2d(net, 128, 2, activation='relu')\n",
    "net = max_pool_2d(conv2, 5)\n",
    "\n",
    "conv3 = conv_2d(net, 128, 1, activation='relu')\n",
    "net = dropout(conv3, 0.5)\n",
    "\n",
    "conv_4 = conv_2d(net, 128, 1, activation='relu', name='conv_3')\n",
    "net = max_pool_2d(conv_4, 2)\n",
    "\n",
    "# Fully-connected 512 node layer\n",
    "network = fully_connected(net, 384, activation='relu')\n",
    "# Dropout layer to combat overfitting\n",
    "network = dropout(network, 0.5)\n",
    "\n",
    "## output layer\n",
    "output = fully_connected(network, 2, activation='softmax')\n",
    "acc = Accuracy(name='Accuracy')\n",
    "output = regression(output,\n",
    "                    optimizer='adam',\n",
    "                    learning_rate=0.0005, \n",
    "                    loss='categorical_crossentropy',\n",
    "                    metric=acc)\n",
    "\n",
    "model = tflearn.DNN(output, checkpoint_path='conv_grey_net.tflearn', max_checkpoints = 3,\n",
    "                    tensorboard_verbose = 3, tensorboard_dir='tmp/tflearn_logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.34927\u001b[0m\u001b[0m | time: 44.649s\n",
      "| Adam | epoch: 100 | loss: 0.34927 - Accuracy: 0.8470 -- iter: 1200/1600\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.34983\u001b[0m\u001b[0m | time: 60.809s\n",
      "| Adam | epoch: 100 | loss: 0.34983 - Accuracy: 0.8458 | val_loss: 0.62666 - val_acc: 0.6775 -- iter: 1600/1600\n",
      "--\n",
      "CPU times: user 5h 11min 24s, sys: 34min 59s, total: 5h 46min 23s\n",
      "Wall time: 1h 46min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, validation_set=(X_test, y_test), batch_size = 400, n_epoch = 100, show_metric=True)\n",
    "              #,verbose = 0, shuffle=True, \n",
    "              #callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tflearn/tflearn/blob/master/examples/basics/weights_persistence.py\n",
    "model.save('tf_cat_dog_grey_conv_net.tfl')  # manually creates a tflearn file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
